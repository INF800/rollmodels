{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "optimize.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOcocKecqUc1HTW1+RekLDy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rakesh4real/role-models/blob/main/optimize-TF-svavedmodels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_G-uSF0VTSyr"
      },
      "source": [
        "- **Tool:** [TF Graph Transforms python API](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms)\n",
        "- **Input:** `SavedModel` [format](https://www.tensorflow.org/guide/saved_model) combines a `GraphDef` with checkpoint files that store weights, **all collected in a folder**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytx5Tp81VOoM"
      },
      "source": [
        "# Steps\n",
        "\n",
        "1. Freeze the `SavedModel` model by converting to `Graphdef` format\n",
        "2. Optimize frozen `GraphDef` mode;\n",
        "3. Unfreeze to `SavedModel` format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWv3P4lLVolT",
        "outputId": "8fc7f2a6-389e-415c-b9a9-5a8204009d9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import sys\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import data\n",
        "from tensorflow.python.saved_model import tag_constants\n",
        "from tensorflow.python.tools import freeze_graph\n",
        "from tensorflow.python import ops\n",
        "from tensorflow.tools.graph_transforms import TransformGraph\n",
        "\n",
        "NUM_CLASSES = 10\n",
        "MODELS_LOCATION = 'models/mnist'\n",
        "MODEL_NAME = 'keras_classifier'\n",
        "\n",
        "def load_mnist_keras():\n",
        "  (train_data, train_labels), (eval_data, eval_labels) = tf.keras.datasets.mnist.load_data()\n",
        "  return train_data, train_labels, eval_data, eval_labels\n",
        "\n",
        "def keras_model_fn(params):\n",
        "    \n",
        "  inputs = tf.keras.layers.Input(shape=(28, 28), name='input_image')\n",
        "  input_layer = tf.keras.layers.Reshape(target_shape=(28, 28, 1), name='reshape')(inputs)\n",
        "  \n",
        "  # convolutional layers\n",
        "  conv_inputs = input_layer\n",
        "  for i in range(params.num_conv_layers):      \n",
        "    filters = params.init_filters * (2**i)\n",
        "    conv = tf.keras.layers.Conv2D(kernel_size=3, filters=filters, strides=1, padding='SAME', activation='relu')(conv_inputs)\n",
        "    max_pool = tf.keras.layers.MaxPool2D(pool_size=2, strides=2, padding='SAME')(conv)\n",
        "    batch_norm = tf.keras.layers.BatchNormalization()(max_pool)\n",
        "    conv_inputs = batch_norm\n",
        "\n",
        "  flatten = tf.keras.layers.Flatten(name='flatten')(conv_inputs)\n",
        "  \n",
        "  # fully-connected layers\n",
        "  dense_inputs = flatten\n",
        "  for i in range(len(params.hidden_units)):      \n",
        "    dense = tf.keras.layers.Dense(units=params.hidden_units[i], activation='relu')(dense_inputs)\n",
        "    dropout = tf.keras.layers.Dropout(params.dropout)(dense)\n",
        "    dense_inputs = dropout\n",
        "      \n",
        "  # softmax classifier\n",
        "  logits = tf.keras.layers.Dense(units=NUM_CLASSES, name='logits')(dense_inputs)\n",
        "  softmax = tf.keras.layers.Activation('softmax', name='softmax')(logits)\n",
        "\n",
        "  # keras model\n",
        "  model = tf.keras.models.Model(inputs, softmax)\n",
        "  return model\n",
        "\n",
        "\n",
        "def create_estimator_keras(params, run_config):\n",
        "    \n",
        "  keras_model = keras_model_fn(params)\n",
        "  print(keras_model.summary())\n",
        "  \n",
        "  optimizer = tf.keras.optimizers.Adam(lr=params.learning_rate)\n",
        "  keras_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  mnist_classifier = tf.keras.estimator.model_to_estimator(\n",
        "      keras_model=keras_model,\n",
        "      config=run_config\n",
        "  )\n",
        "  \n",
        "  return mnist_classifier"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e8a090e77c56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfreeze_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_transforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTransformGraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mNUM_CLASSES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.tools.graph_transforms'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgLwArQUWQUo"
      },
      "source": [
        "def run_experiment(hparams, train_data, train_labels, run_config, create_estimator_fn=create_estimator):\n",
        "  train_spec = tf.estimator.TrainSpec(\n",
        "      input_fn = tf.estimator.inputs.numpy_input_fn(\n",
        "          x={'input_image': train_data},\n",
        "          y=train_labels,\n",
        "          batch_size=hparams.batch_size,\n",
        "          num_epochs=None,\n",
        "          shuffle=True),\n",
        "      max_steps=hparams.max_training_steps\n",
        "  )\n",
        "  eval_spec = tf.estimator.EvalSpec(\n",
        "      input_fn = tf.estimator.inputs.numpy_input_fn(\n",
        "          x={'input_image': train_data},\n",
        "          y=train_labels,\n",
        "          batch_size=hparams.batch_size,\n",
        "          num_epochs=1,\n",
        "          shuffle=False),\n",
        "      steps=None,\n",
        "      throttle_secs=hparams.eval_throttle_secs\n",
        "  )\n",
        "\n",
        "  tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "  time_start = datetime.utcnow()\n",
        "  print('Experiment started at {}'.format(time_start.strftime('%H:%M:%S')))\n",
        "  print('.......................................')\n",
        "\n",
        "  estimator = create_estimator_fn(hparams, run_config)\n",
        "\n",
        "  tf.estimator.train_and_evaluate(\n",
        "      estimator=estimator,\n",
        "      train_spec=train_spec,\n",
        "      eval_spec=eval_spec\n",
        "  )\n",
        "\n",
        "  time_end = datetime.utcnow()\n",
        "  print('.......................................')\n",
        "  print('Experiment finished at {}'.format(time_end.strftime('%H:%M:%S')))\n",
        "  print('')\n",
        "  time_elapsed = time_end - time_start\n",
        "  print('Experiment elapsed time: {} seconds'.format(time_elapsed.total_seconds()))\n",
        "\n",
        "  return estimator\n",
        "\n",
        "\n",
        "def train_and_export_model(train_data, train_labels):\n",
        "  model_dir = os.path.join(MODELS_LOCATION, MODEL_NAME)\n",
        "\n",
        "  hparams  = tf.contrib.training.HParams(\n",
        "      batch_size=100,\n",
        "      hidden_units=[1024],\n",
        "      num_conv_layers=2,\n",
        "      init_filters=64,\n",
        "      dropout=0.85,\n",
        "      max_training_steps=50,\n",
        "      eval_throttle_secs=10,\n",
        "      learning_rate=1e-3,\n",
        "      debug=True\n",
        "  )\n",
        "\n",
        "  run_config = tf.estimator.RunConfig(\n",
        "      tf_random_seed=19830610,\n",
        "      save_checkpoints_steps=1000,\n",
        "      keep_checkpoint_max=3,\n",
        "      model_dir=model_dir\n",
        "  )\n",
        "\n",
        "  if tf.gfile.Exists(model_dir):\n",
        "      print('Removing previous artifacts...')\n",
        "      tf.gfile.DeleteRecursively(model_dir)\n",
        "\n",
        "  os.makedirs(model_dir)\n",
        "\n",
        "  estimator = run_experiment(hparams, train_data, train_labels, run_config, create_estimator_keras)\n",
        "\n",
        "  def make_serving_input_receiver_fn():\n",
        "      inputs = {'input_image': tf.placeholder(\n",
        "          shape=[None,28,28], dtype=tf.float32, name='serving_input_image')}\n",
        "      return tf.estimator.export.build_raw_serving_input_receiver_fn(inputs)\n",
        "\n",
        "  export_dir = os.path.join(model_dir, 'export')\n",
        "\n",
        "  if tf.gfile.Exists(export_dir):\n",
        "      tf.gfile.DeleteRecursively(export_dir)\n",
        "\n",
        "  estimator.export_savedmodel(\n",
        "      export_dir_base=export_dir,\n",
        "      serving_input_receiver_fn=make_serving_input_receiver_fn()\n",
        "  )\n",
        "\n",
        "  return export_dir\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}