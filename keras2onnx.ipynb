{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "keras2onnx.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMFhEBddpw4tRd0oju2Xw+l",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rakesh4real/rollmodels/blob/main/keras2onnx.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67bo14f6L4XC"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reERZLldLrnQ",
        "outputId": "6100caf2-4df6-486c-aaea-03867f44e5a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# original source: https://github.com/onnx/keras-onnx/tree/master/tutorial\n",
        "# Note: this code is compact and better compared to original\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "## 1. Setup\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "# 2. Random Image\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def get_and_plot_random_image_from(x_test, y_test):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    # get a random image index from the test set\n",
        "    image_index = int(np.random.randint(0, x_test.shape[0], size=1)[0])\n",
        "    expected_label = y_test[image_index]\n",
        "    digit_image = x_test[image_index]\n",
        "    # and plot it\n",
        "    plt.title(f'Example {image_index} Label: {expected_label}')\n",
        "    plt.imshow(digit_image.squeeze(2), cmap='Greys')\n",
        "    plt.show()\n",
        "    return digit_image, image_index\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "# 3. Inference w/o onnx\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def infer_w_raw_h5(digit_image, expected_label, model, input_shape, loop_count=100):\n",
        "    # reshape the image for inference/prediction\n",
        "    #digit_image = digit_image.reshape(1, input_shape[0], input_shape[1], 1)\n",
        "    digit_image = digit_image.reshape(1, input_shape[0], input_shape[1], input_shape[2])\n",
        "\n",
        "    # loop `loop_count` times\n",
        "    start_time = time.time()\n",
        "    for i in range(loop_count):\n",
        "        prediction_probabs = model.predict(digit_image)\n",
        "    avg_time = ((time.time() - start_time) / loop_count)\n",
        "    print(f\"Keras inferences with {avg_time} second in average\")\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"Prediction probabilities:\")\n",
        "    print(prediction_probabs)\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    predicted_label = prediction_probabs.argmax()\n",
        "    print('Predicted value:', predicted_label)\n",
        "\n",
        "    is_correct_pred = None\n",
        "    if (expected_label.argmax() == predicted_label):\n",
        "      print('Correct prediction')\n",
        "      is_correct_pred = True\n",
        "    else:\n",
        "      print('Wrong prediction')\n",
        "      is_correct_pred = False\n",
        "\n",
        "    # can be used for more control (see suggestio where used)\n",
        "    return (is_correct_pred, prediction_probabs), avg_time\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "# 4. Inference w/ onnx\n",
        "\n",
        "!pip install --quiet -U onnxruntime\n",
        "!pip install --quiet -U git+https://github.com/microsoft/onnxconverter-common\n",
        "!pip install --quiet -U git+https://github.com/onnx/keras-onnx\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "!pip install --quiet -U onnxruntime\n",
        "!pip install --quiet -U git+https://github.com/microsoft/onnxconverter-common\n",
        "!pip install --quiet -U git+https://github.com/onnx/keras-onnx\n",
        "\n",
        "import onnxruntime\n",
        "import keras2onnx\n",
        "\n",
        "def h52onnx(model, output_onnxmodel_path='./onnx_model.onnx'):\n",
        "  print(\"keras2onnx version is \"+keras2onnx.__version__)\n",
        "  save_name = output_onnxmodel_path.split(\".onnx\")[0].split(\"/\")[-1] # get name from path\n",
        "  print(save_name)\n",
        "  # convert to onnx model\n",
        "  onnx_model = keras2onnx.convert_keras(model, save_name, debug_mode=1)\n",
        "  # and save the model in ONNX format\n",
        "  keras2onnx.save_model(onnx_model, output_onnxmodel_path)\n",
        "\n",
        "\n",
        "def generate_data_feed_and_sess(digit_image, input_shape, output_onnxmodel_path):\n",
        "    # reshape the image for inference/prediction\n",
        "    digit_image = digit_image.reshape(1, input_shape[0], input_shape[1], 1)\n",
        "    \n",
        "    # define session (include options using docs if necessary)\n",
        "    sess_options = onnxruntime.SessionOptions()\n",
        "    sess = onnxruntime.InferenceSession(output_onnxmodel_path, sess_options)\n",
        "    # define data\n",
        "    data = [digit_image.astype(np.float32)]\n",
        "    # feed data\n",
        "    input_names = sess.get_inputs()\n",
        "    feed = dict([(input.name, data[n]) for n, input in enumerate(sess.get_inputs())])\n",
        "    return feed, sess\n",
        "    \n",
        "\n",
        "def infer_w_onnx_runtime(digit_image, expected_label,\n",
        "                         input_shape,\n",
        "                         output_onnxmodel_path='./onnx_model.onnx',\n",
        "                         loop_count=100):\n",
        "  # setup\n",
        "  feed, sess = generate_data_feed_and_sess(digit_image, input_shape, output_onnxmodel_path)\n",
        "\n",
        "  # calculate average\n",
        "  start_time = time.time()\n",
        "  for i in range(loop_count):\n",
        "      # note: session is used to make preds\n",
        "      onnx_prediction_probabs = sess.run(None, feed)[0]\n",
        "  avg_time = ((time.time() - start_time) / loop_count)\n",
        "  print(f\"ONNX inferences with {avg_time} second in average\")\n",
        "\n",
        "  # compare w/ raw h5\n",
        "  print(\"=\"*60)\n",
        "  print(\"[onnx]Prediction probabilities:\")\n",
        "  print(onnx_prediction_probabs)\n",
        "  print(\"=\"*60)\n",
        "\n",
        "  is_correct_pred = None\n",
        "  print('ONNX predicted value:', onnx_prediction_probabs.argmax())\n",
        "  if (expected_label.argmax() == onnx_prediction_probabs.argmax()):\n",
        "    print('Correct prediction')\n",
        "    is_correct_pred = True\n",
        "  else:\n",
        "    print('Wrong prediction')\n",
        "    is_correct_pred = False\n",
        "\n",
        "  return (is_correct_pred, onnx_prediction_probabs), avg_time"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for onnxconverter-common (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for keras2onnx (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQoS0fs-qoCa"
      },
      "source": [
        "# **Part 1:** Lighter Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiVPGdNYCVRl"
      },
      "source": [
        "# Create Keras .h5 Model\n",
        "\n",
        "- Makes sure input dims is given in first layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jd80L3VIgmBw"
      },
      "source": [
        "input_shape = (28, 28, 1)\n",
        "\n",
        "# Important while creating keras model, making inference w and w/o keras (onnx)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26E1zR2P_cUX",
        "outputId": "f860ff38-dd8c-4f39-c8eb-cf3d63a753af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "source": [
        "\"\"\"\n",
        "## Setup\n",
        "\"\"\"\n",
        "import os\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow\n",
        "print(\"TensorFlow version is \"+tensorflow.__version__)\n",
        "\n",
        "\n",
        "SEED = 101\n",
        "from numpy.random import seed\n",
        "seed(101)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(101)\n",
        "\n",
        "\"\"\"\n",
        "## Prepare the data\n",
        "\"\"\"\n",
        "\n",
        "# Model / data parameters\n",
        "num_classes = 10\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Scale images to the [0, 1] range\n",
        "x_train = x_train.astype(\"float32\") / 255\n",
        "x_test = x_test.astype(\"float32\") / 255\n",
        "# Make sure images have shape (28, 28, 1)\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(x_train.shape[0], \"train samples\")\n",
        "print(x_test.shape[0], \"test samples\")\n",
        "\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = tensorflow.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tensorflow.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\"\"\"\n",
        "## Build the model\n",
        "\"\"\"\n",
        "\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=input_shape), # !NOTE: input shape must be defined @beg to avoid onnx errors\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\"\"\"\n",
        "## Train the model\n",
        "\"\"\"\n",
        "\n",
        "os.makedirs(\"keras_saved_model\", exist_ok=True)\n",
        "checkpoint_filepath = 'keras_saved_model/model.h5'\n",
        "model_checkpoint_callback = tensorflow.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=False, # else, error\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "\n",
        "\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 10\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=[model_checkpoint_callback])\n",
        "\n",
        "\"\"\"\n",
        "## Evaluate the trained model\n",
        "\"\"\"\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow version is 2.3.0\n",
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_2 (Conv2D)            (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                16010     \n",
            "=================================================================\n",
            "Total params: 34,826\n",
            "Trainable params: 34,826\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "422/422 [==============================] - 39s 92ms/step - loss: 0.3707 - accuracy: 0.8863 - val_loss: 0.0908 - val_accuracy: 0.9767\n",
            "Epoch 2/10\n",
            "422/422 [==============================] - 39s 92ms/step - loss: 0.1206 - accuracy: 0.9632 - val_loss: 0.0607 - val_accuracy: 0.9835\n",
            "Epoch 3/10\n",
            "422/422 [==============================] - 39s 92ms/step - loss: 0.0864 - accuracy: 0.9735 - val_loss: 0.0486 - val_accuracy: 0.9873\n",
            "Epoch 4/10\n",
            "422/422 [==============================] - 39s 92ms/step - loss: 0.0736 - accuracy: 0.9774 - val_loss: 0.0445 - val_accuracy: 0.9868\n",
            "Epoch 5/10\n",
            "422/422 [==============================] - 39s 92ms/step - loss: 0.0625 - accuracy: 0.9810 - val_loss: 0.0379 - val_accuracy: 0.9897\n",
            "Epoch 6/10\n",
            "422/422 [==============================] - 39s 92ms/step - loss: 0.0562 - accuracy: 0.9826 - val_loss: 0.0385 - val_accuracy: 0.9888\n",
            "Epoch 7/10\n",
            "422/422 [==============================] - 39s 92ms/step - loss: 0.0523 - accuracy: 0.9841 - val_loss: 0.0345 - val_accuracy: 0.9913\n",
            "Epoch 8/10\n",
            "422/422 [==============================] - 42s 100ms/step - loss: 0.0465 - accuracy: 0.9859 - val_loss: 0.0358 - val_accuracy: 0.9908\n",
            "Epoch 9/10\n",
            "422/422 [==============================] - 39s 92ms/step - loss: 0.0455 - accuracy: 0.9858 - val_loss: 0.0343 - val_accuracy: 0.9912\n",
            "Epoch 10/10\n",
            "422/422 [==============================] - 39s 92ms/step - loss: 0.0415 - accuracy: 0.9868 - val_loss: 0.0319 - val_accuracy: 0.9912\n",
            "Test loss: 0.030177347362041473\n",
            "Test accuracy: 0.9896000027656555\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwfkAOeEDfr_"
      },
      "source": [
        "# Load and eval best.h5 model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXOnOTUQDJwr",
        "outputId": "bfdc1ae0-7886-45fa-e9ff-9f53f232a685",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# best models\n",
        "model = tensorflow.keras.models.load_model(checkpoint_filepath)\n",
        "\n",
        "# eval\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.030294399708509445\n",
            "Test accuracy: 0.9894999861717224\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8gXaGUhHZ3a",
        "outputId": "c6e2fb08-7c67-4308-b5dd-675b20c0d30d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        }
      },
      "source": [
        "# a. get random image\n",
        "digit_image, digit_idx = get_and_plot_random_image_from(x_test, y_test)\n",
        "\n",
        "# b. loop inference for avg. inference time\n",
        "(_, keras_prbs_100it), keras_time_100it = infer_w_raw_h5(digit_image, expected_label=y_test[digit_idx],\n",
        "                                                              model=model, input_shape=input_shape)\n",
        "\n",
        "# SUGGESTION: For more control, as this is for single, image, \n",
        "# Go one step further by putiing the above two functions \n",
        "# in loop as well and take average"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAS4AAAEICAYAAADhtRloAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAWZ0lEQVR4nO3de5BcdZnG8e9DErkFNZghFa4BlosgENgR3UIUlsvirSCyUOAuhtpA0ILFLKxIUSJZRAtcQZBF3CB3CHIJAdYFVgQDpKLIBBECUQyYkGBIBiImgXAJefePc8Y6GbrP9Myk5/Rv5vlUdaX7vOfX/fave545fc6ZjiICM7OUbFR1A2ZmveXgMrPkOLjMLDkOLjNLjoPLzJLj4DKz5AyK4JJ0oqTZVfdRBUlTJd3U7LH5HL8rabWkD/fl8czKSHpe0tuNvCd7DC5JCyWtyd+wXZf/2jCtVk/SsZLmSHpD0qwa9ZD0euG5/7jGOu+TNF/SksKyA7vN2er8vo6u08d1ki7YoE9uw/tlRIyMiPldCyT9m6SXJa2UdI2kjRu9M0lflLQon9+7JG3Zi7GHSPpd/rr9QtIOvRg7XtLcfOxcSeN7MfZbkp6WtFbS1EbH5WM3zudoZT5nZ/RirCRdJOnV/HKRJPVifMu/ThGxM/CdRu630S2uz+dv2K7LaQ2OS8EK4FLgwpJ19ik895Nq1L8GdBYXRMSjxTkDPgesBu7fUI1XTdI/AGcDhwA7ADsB/9Hg2D2B/wZOAMYAbwA/bHDsaOBO4FxgS6ADuLXBse8D7gZuAkYB1wN358sbsQA4C/jfBtcvmgrsQjZXBwNnSTqiwbGTgaOAfYC9gc8DpzQyMMXXqUcRUXoBFgKH1qldCcwo3L4IeBAQ2Zvip2Q/0H/Or29bWHcWcAEwh+wH+n+ADwE3AyuBx4FxhfUDOB14AXgF+E9go7x2IjC7sO7uwANkofR74NgGnudJwKwaywP4m5JxOwLzgU8DS0rWuxa4tqR+HXBBndplwOJ8XuYCBxZqU4E7yN4Qq4AnyIK2q741MCN/Hf4InN5t7E09zU2tOc6XTQe+U7h9CPByg/f3HWB64fbOwNvAFg2MnQzMKdzeHFgD7N7A2MOBlwAVlr0IHNFI34UxNwFTeznmT8DhhdvfAn7S4Ng5wOTC7UnArxocm8zr1Oh7sr/7uM4E9sr3fxyYT+bEyDrYiOyHdQdg+7zh7h8xjyNL8m3yCfllPmZLsjA4r9v6E4B2YD/gSOBfujckaXOy0JoObJU/xg8l7dGP5/lIvpl9p6Rx3WqXA+eQPb+a8p7+key3e188Downm5fpwO2SNinUjwRuL9TvkjRC0kZkvxB+SzbHhwBT8t/Atfp8StIXe9HXnvl9d/ktMEbSh3o7NiKeJ/uB2LUPY18Hns+XNzL2qfw92uWpBsf2maRRwFjeO1+NPm6tue7P2FZ/nUo1Glx3SXqtcDk5b+QNsuC5hOw30L9GxJK89mpEzIiINyJiFfBt4FPd7vfaiHg+Iv4C3Ac8HxE/j4i1ZD+I+3Zb/6KIWBERL5J9vDu+Rq+fAxZGxLURsTYifkO2xXFMg8+1u08B48i24v4E/FTScABJE4BhETGzh/v4AtlW4sN9aSAibsrnc21EXAxsDOxWWGVuRNwREe+QvRabAB8HPgq0RcT5EfF2RLwAXEUW5rUeZ++ImN6L1kYCfync7rq+RR/Gdo1v5bH9MbLwWH153FpzPbLB/Vwpvk6lhje43lER8fNahYh4TNILZFs3t3Utl7QZ8H3gCLKPjQBbSBoWEe/mt5cV7mpNjdsjWd/iwvVFZB+DutsB+Jik1wrLhgM31uq/JxHxSH71bUlfJfu49uH8OX8X+EwDdzMRuKHbb/mGSfp3sq3Zrck+ur4fGF1Y5a/zEhHr8oMEXetu3W0uhgGP9qWPGlbnvXTpur6qD2O7xrfy2P5YXXisN/vwuLXmenWD76kUX6dS/T4dQtKpZFsAfyLbadnlTLKtgo9FxPuBT3YN6cfDbVe4vn3+mN0tBh6OiA8WLiMj4iv9eNyiIHsOu5BtiT0q6WWynZBj84+U47pWlrQdcBBwQ18eLP8IfhZwLDAqIj5I9lurOI/bFdbfCNiWbG4WA3/sNhdbREQjYduIZ8h2FnfZB1gWEa/2dqykncjeR8/1YezmZLsanmlw7N7dtlT2bnBsn0XEn4GlvHe+Gn3cWnPdn7Gt/jqV6ldwSdqVbAf7P5N9ZDyrcGh5C7Ktptfyw6fd91f1xdckjcrD4KvUPkLxU2BXSSfk+3lGSPqo6px7JGlYvr9oOLCRpE0kjchre+aHzodJGglcTLZjdz4wjywwxueXk8i2GMez/pbhCWQ7KJ9v4PkNyx+/6/I+snlcS7Zzfbikb/Le32J/K+kL+UfYKcBbwK+AXwOrJH1d0qb58/iIpI820EsjbgAmSdpD0geBb5AdZGjEzcDnlZ02sjlwPnBnvluhJzOBj0g6On/tvkm23+p3DYydBbwLnJ6fntB1hPyhRprO30+bkP3sDM9fp2GNjCWbr2/k7+HdgZNpfL5uAM6QtI2krck2DHozNrXXqVwDRwYWkgXQ6sJlJtkP+q+BswvrfgV4miyRtyZ7k6wmS+dTyLZWhufrzgJOKoy9ALiucPtQYEHhdvGo4qtkITIsr53I+kcVdyM7XN2Zr/sQML7O8zsxv+/i5bq89vdkRyVfB5YDdwG71Lmfg6hxVBH4HTCpgXm+rkYfs8k+2l1D9hF1KdnW10LyI72896jib4D9Cve7NXAL8DLZ0d1fdRt7U2HdZ4B/Kpmn2TWWn0EW2CvJDqxs3Mj95fUvkh3Re53sFIUtC7X7gHNKxh6az+2a/L00rlD7EfCjkrH7kh2dXUN2FHbfQu0c4L5evk4n5rUDyT6+1Ru7ceG1XAacUahtT/azsn2dsSLbNbEiv3yX9Y+MrqZwtDnV16n7e7LeRfnKLU9SkIXGgqp7GYoknUB2Ps/bwN9F4SRUsw1B0u/Jjn7fFhHvOWNgvXUdXGaWmkHxt4pmNrQks8VlZtbFW1xmlpxGT0Ct3OjRo2PcuHFVt2E2qM2dO/eViGiruo+eVBpc+V/GX0Z2yP/HEVH3GxrGjRtHR0fHgPVmNhRJWlR1D42o7KNiftLeFWTfqrAHcHw//xDazIaIKvdx7U92gukLEfE28BOybzkwMytVZXBtw/p/GrMkX/ZXkiZL6pDU0dm53vf0mdkQ1tJHFSNiWkS0R0R7W1vL7y80swFSZXC9xPrf9rBtvszMrFSVwfU4sIukHfNvQTgOuKfCfswsEZWdDhERa/OvFPk/8m9AiIimfieSmQ0OlZ7HFRH3AvdW2YOZpaeld86bmdXi4DKz5Di4zCw5Di4zS46Dy8yS4+Ays+Q4uMwsOQ4uM0uOg8vMkuPgMrPkOLjMLDkOLjNLjoPLzJLj4DKz5Di4zCw5Di4zS46Dy8yS4+Ays+Q4uMwsOQ4uM0uOg8vMkuPgMrPkOLjMLDkOLjNLjoPLzJLj4DKz5Di4zCw5Di4zS46Dy8ySM7zqBqy51q5dW1rfaqut6taWL19eOnb4cL99rBqVvvMkLQRWAe8CayOivcp+zCwNrfAr8+CIeKXqJswsHd7HZWbJqTq4AviZpLmSJncvSposqUNSR2dnZwXtmVkrqjq4PhER+wGfBk6V9MliMSKmRUR7RLS3tbVV06GZtZxKgysiXsr/XQ7MBPavsh8zS0NlwSVpc0lbdF0HDgfmVdWPmaWjyqOKY4CZkrr6mB4R91fYz6B0zz33lNZfe+21urU77rijdOxxxx3Xp54GQk/nry1YsKC0vvvuu2/IdmwDqyy4IuIFYJ+qHt/M0lX1znkzs15zcJlZchxcZpYcB5eZJcfBZWbJaYU/srYWdeutt5bWW/l0iHXr1pXWH3jggdK6T4dobd7iMrPkOLjMLDkOLjNLjoPLzJLj4DKz5Di4zCw5Di4zS47P47K63nrrrapbqKvs63gAJk2aVFq/7777SuuHHXZY3ZrP8aqet7jMLDkOLjNLjoPLzJLj4DKz5Di4zCw5Di4zS46Dy8yS4/O4BrnLL7+8tD58eP23QE//PVmVejoPa+bMmf26/87Ozro1n8dVPW9xmVlyHFxmlhwHl5klx8FlZslxcJlZchxcZpYcB5eZJcfncQ1yixYtKq1LqlvbbLPNNnQ7vbJ06dK6tfvvv38AO7FW0/QtLknXSFouaV5h2ZaSHpD0h/zfUc3uw8wGj4H4qHgdcES3ZWcDD0bELsCD+W0zs4Y0Pbgi4hFgRbfFRwLX59evB45qdh9mNnhUtXN+TER07cB4GRhTayVJkyV1SOoo+9sxMxtaKj+qGBEBRJ3atIhoj4j2tra2Ae7MzFpVVcG1TNJYgPzf5RX1YWYJqiq47gEm5tcnAndX1IeZJajp53FJugU4CBgtaQlwHnAhcJukScAi4Nhm9zFYzZkzp7Te03lcEyZM2JDtrKen//vw/PPPL60/++yzdWsdHR196qnLaaedVlo/4IAD+nX/1lxND66IOL5O6ZBmP7aZDU6V75w3M+stB5eZJcfBZWbJcXCZWXIcXGaWHH+tTeLuvrv8FLjsDxPqO/jgg/v82CtWdP8T1PX1dEpB2X+NBrB8ef3zknt67JEjR5bWf/CDH5TWrbV5i8vMkuPgMrPkOLjMLDkOLjNLjoPLzJLj4DKz5Di4zCw5Po8rcbNnz+7X+GXLltWtzZgxo3TsueeeW1pfvHhxaf2YY44prT/00EOl9TJf+tKX+jzWWp+3uMwsOQ4uM0uOg8vMkuPgMrPkOLjMLDkOLjNLjoPLzJLj87iGuAsuuKBp9/3Zz362tH7ttdeW1vfbb7+6tZ7OETvqqKNK65Y2b3GZWXIcXGaWHAeXmSXHwWVmyXFwmVlyHFxmlhwHl5klx+dxWZ/tueeepfXddtuttL5gwYLS+nPPPdfrnrr05/+LtNbX9C0uSddIWi5pXmHZVEkvSXoyv3ym2X2Y2eAxEB8VrwOOqLH8+xExPr/cOwB9mNkg0fTgiohHgPL/L93MrBeq3Dl/mqSn8o+So2qtIGmypA5JHZ2dnQPdn5m1qKqC60pgZ2A8sBS4uNZKETEtItojor2trW0g+zOzFlZJcEXEsoh4NyLWAVcB+1fRh5mlqZLgkjS2cHMCMK/eumZm3TX9PC5JtwAHAaMlLQHOAw6SNB4IYCFwSrP7GKyuuOKK0vqBBx5YWn/99dfr1jbddNPSsbfffntpfccddyytf/nLXy6tl/XW3t5eOlZSad3S1vTgiojjayy+utmPa2aDl//kx8yS4+Ays+Q4uMwsOQ4uM0uOg8vMkuOvtUnc+PHjS+tz5swprU+dOrVu7cYbbywdu9lmm5XWFy5cWFqfPn16ab3slIajjz66dOxGG/l38mDmV9fMkuPgMrPkOLjMLDkOLjNLjoPLzJLj4DKz5Di4zCw5Po9rkNtrr71K62VfTdPfc6HefPPN0vo777zT5/seMWJEn8da+rzFZWbJcXCZWXIcXGaWHAeXmSXHwWVmyXFwmVlyHFxmlhyfxzXENfN7qy688MJ+jT/88MPr1k4//fR+3belzVtcZpYcB5eZJcfBZWbJcXCZWXIcXGaWHAeXmSXHwWVmyWnqeVyStgNuAMYAAUyLiMskbQncCowDFgLHRsSfm9mLbXiLFy8urc+aNatf93/ooYfWrfn7uIa2Zm9xrQXOjIg9gI8Dp0raAzgbeDAidgEezG+bmTWkqcEVEUsj4on8+ipgPrANcCRwfb7a9cBRzezDzAaXAdvHJWkcsC/wGDAmIpbmpZfJPkqamTVkQIJL0khgBjAlIlYWaxERZPu/ao2bLKlDUkdnZ+cAdGpmKWh6cEkaQRZaN0fEnfniZZLG5vWxwPJaYyNiWkS0R0R7W1tbs1s1s0Q0NbgkCbgamB8RlxRK9wAT8+sTgbub2YeZDS7N/lqbA4ATgKclPZkvOwe4ELhN0iRgEXBsk/uwJrj00ktL6y+++GJpfauttiqtn3zyyb3uyYaGpgZXRMwGVKd8SDMf28wGL585b2bJcXCZWXIcXGaWHAeXmSXHwWVmyXFwmVly/N+TWV1r1qwprd98882l9Z7O03r44YdL6x/4wAdK6zZ0eYvLzJLj4DKz5Di4zCw5Di4zS46Dy8yS4+Ays+Q4uMwsOT6Pa4hbt25d3dqUKVNKx65du7a0PmfOnNL6rrvuWlo3q8dbXGaWHAeXmSXHwWVmyXFwmVlyHFxmlhwHl5klx8FlZsnxeVxDXETUrd1yyy2lY5988snS+k477dSnnsx64i0uM0uOg8vMkuPgMrPkOLjMLDkOLjNLjoPLzJLj4DKz5DT1PC5J2wE3AGOAAKZFxGWSpgInA535qudExL3N7MVqGzZsWN3aypUrB7ATs8Y1+wTUtcCZEfGEpC2AuZIeyGvfj4jvNfnxzWwQampwRcRSYGl+fZWk+cA2zXxMMxv8Bmwfl6RxwL7AY/mi0yQ9JekaSaPqjJksqUNSR2dnZ61VzGwIGpDgkjQSmAFMiYiVwJXAzsB4si2yi2uNi4hpEdEeEe1tbW0D0aqZJaDpwSVpBFlo3RwRdwJExLKIeDci1gFXAfs3uw8zGzyaGlySBFwNzI+ISwrLxxZWmwDMa2YfZja4NPuo4gHACcDTkrq+A+Uc4HhJ48lOkVgInNLkPsxsEGn2UcXZgGqUfM6WmfWZz5w3s+Q4uMwsOQ4uM0uOg8vMkuPgMrPkOLjMLDkOLjNLjoPLzJLj4DKz5Di4zCw5Di4zS46Dy8yS4+Ays+Q4uMwsOYqIqntoiKROYFFh0WjglYra6Yl765tW7a1V+4IN39sOEdHy35OeTHB1J6kjItqr7qMW99Y3rdpbq/YFrd1bM/mjopklx8FlZslJObimVd1ACffWN63aW6v2Ba3dW9Mku4/LzIaulLe4zGyIcnCZWXKSDC5JR0j6vaQFks6uup8iSQslPS3pSUkdFfdyjaTlkuYVlm0p6QFJf8j/HdUifU2V9FI+b09K+sxA95X3sZ2kX0h6VtIzkr6aL2+FeavXW0vM3UBKbh+XpGHAc8BhwBLgceD4iHi20sZykhYC7RFR+QmLkj4JrAZuiIiP5Mu+C6yIiAvz0B8VEV9vgb6mAqsj4nsD2UuN3sYCYyPiCUlbAHOBo4ATqX7e6vV2LC0wdwMpxS2u/YEFEfFCRLwN/AQ4suKeWlJEPAKs6Lb4SOD6/Pr1ZG/8AVWnr5YQEUsj4on8+ipgPrANrTFv9XobclIMrm2AxYXbS2itFy+An0maK2ly1c3UMCYilubXXwbGVNlMN6dJeir/KDngH8W6kzQO2Bd4jBabt269QYvNXbOlGFyt7hMRsR/waeDU/GNRS4psP0Gr7Cu4EtgZGA8sBS6ushlJI4EZwJSIWFmsVT1vNXprqbkbCCkG10vAdoXb2+bLWkJEvJT/uxyYSfbRtpUsy/eVdO0zWV5xPwBExLKIeDci1gFXUeG8SRpBFgw3R8Sd+eKWmLdavbXS3A2UFIPrcWAXSTtKeh9wHHBPxT0BIGnzfKcpkjYHDgfmlY8acPcAE/PrE4G7K+zlr7pCITeBiuZNkoCrgfkRcUmhVPm81eutVeZuICV3VBEgP9x7KTAMuCYivl1xSwBI2olsKwtgODC9yt4k3QIcRPbVJ8uA84C7gNuA7cm+JujYiBjQHeV1+jqI7KNOAAuBUwr7lAayt08AjwJPA+vyxeeQ7Uuqet7q9XY8LTB3AynJ4DKzoS3Fj4pmNsQ5uMwsOQ4uM0uOg8vMkuPgMrPkOLjMLDkOLjNLzv8D2VGptuzwd1QAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Keras inferences with 0.03900907278060913 second in average\n",
            "============================================================\n",
            "Prediction probabilities:\n",
            "[[1.0713426e-07 2.5377343e-05 2.8030829e-07 1.8252522e-05 9.9827707e-01\n",
            "  2.4302868e-05 1.4926248e-06 2.0018418e-04 1.5880645e-05 1.4370248e-03]]\n",
            "============================================================\n",
            "Predicted value: 4\n",
            "Correct prediction\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RI36w9_PJaqP"
      },
      "source": [
        "# Conversion from Keras to ONNX\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pJG8tvOJeE2",
        "outputId": "5d5a94fd-b450-4372-9ac1-505e61c097d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "h52onnx(model, \"./keras-mnist-optimized.onnx\") # saves in curdir"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf executing eager_mode: True\n",
            "tf.keras model eager_mode: False\n",
            "Processing a keras layer - (dense_1: <class 'tensorflow.python.keras.layers.core.Dense'>)\n",
            "\toutput: dense_1/Softmax_1:0\n",
            "\tinput : dropout_1/cond_1/Identity:0\n",
            "Processing a keras layer - (dropout_1: <class 'tensorflow.python.keras.layers.core.Dropout'>)\n",
            "\toutput: dropout_1/cond_1/Identity:0\n",
            "\tinput : flatten_1/Reshape_1:0\n",
            "Processing a keras layer - (flatten_1: <class 'tensorflow.python.keras.layers.core.Flatten'>)\n",
            "\toutput: flatten_1/Reshape_1:0\n",
            "\tinput : max_pooling2d_3/MaxPool_1:0\n",
            "Processing a keras layer - (max_pooling2d_3: <class 'tensorflow.python.keras.layers.pooling.MaxPooling2D'>)\n",
            "\toutput: max_pooling2d_3/MaxPool_1:0\n",
            "\tinput : conv2d_3/Relu_1:0\n",
            "Processing a keras layer - (conv2d_3: <class 'tensorflow.python.keras.layers.convolutional.Conv2D'>)\n",
            "\toutput: conv2d_3/Relu_1:0\n",
            "\tinput : max_pooling2d_2/MaxPool_1:0\n",
            "Processing a keras layer - (max_pooling2d_2: <class 'tensorflow.python.keras.layers.pooling.MaxPooling2D'>)\n",
            "\toutput: max_pooling2d_2/MaxPool_1:0\n",
            "\tinput : conv2d_2/Relu_1:0\n",
            "Processing a keras layer - (conv2d_2: <class 'tensorflow.python.keras.layers.convolutional.Conv2D'>)\n",
            "\toutput: conv2d_2/Relu_1:0\n",
            "\tinput : input_2_1:0\n",
            "var: input_2\n",
            "var: input_2_1:0\n",
            "var: input_2_1:01\n",
            "var: conv2d_2/Relu_1:0\n",
            "var: max_pooling2d_2/MaxPool_1:0\n",
            "var: conv2d_3/Relu_1:0\n",
            "var: max_pooling2d_3/MaxPool_1:0\n",
            "var: flatten_1/Reshape_1:0\n",
            "var: dropout_1/cond_1/Identity:0\n",
            "var: dense_1/Softmax_1:01\n",
            "var: dense_1/Softmax_1:0\n",
            "var: dense_1\n",
            "Converting the operator (Identity): Identity\n",
            "Converting the operator (Identity1): Identity\n",
            "Converting the operator (Identity2): Identity\n",
            "Converting the operator (dense_1): <class 'tensorflow.python.keras.layers.core.Dense'>\n",
            "Converting the operator (dropout_1): <class 'tensorflow.python.keras.layers.core.Dropout'>\n",
            "Converting the operator (keras_learning_phase/input): Const\n",
            "Converting the operator (flatten_1): <class 'tensorflow.python.keras.layers.core.Flatten'>\n",
            "Converting the operator (flatten_1/Const_1): Const\n",
            "Converting the operator (max_pooling2d_3): <class 'tensorflow.python.keras.layers.pooling.MaxPooling2D'>\n",
            "Converting the operator (conv2d_3): <class 'tensorflow.python.keras.layers.convolutional.Conv2D'>\n",
            "Converting the operator (max_pooling2d_2): <class 'tensorflow.python.keras.layers.pooling.MaxPooling2D'>\n",
            "Converting the operator (conv2d_2): <class 'tensorflow.python.keras.layers.convolutional.Conv2D'>\n",
            "Converting the operator (Identity3): Identity\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "keras2onnx version is 1.7.1\n",
            "keras-mnist-optimized\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_2 (Conv2D)            (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                16010     \n",
            "=================================================================\n",
            "Total params: 34,826\n",
            "Trainable params: 34,826\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8UkDSbKJxIJ"
      },
      "source": [
        "# Evaluate the ONNX's Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOgM8jEvjY02",
        "outputId": "e760adb7-0505-47a8-d5d7-8e8cc0ccea09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "(_, onnxs_prbs_100it), onnx_time_100it = infer_w_onnx_runtime(digit_image, expected_label=y_test[digit_idx], \n",
        "                                                              input_shape=input_shape,\n",
        "                                                              output_onnxmodel_path='./keras-mnist-optimized.onnx') # same path as used above to load"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ONNX inferences with 0.0005920720100402832 second in average\n",
            "============================================================\n",
            "[onnx]Prediction probabilities:\n",
            "[[1.0713405e-07 2.5377294e-05 2.8030775e-07 1.8252522e-05 9.9827707e-01\n",
            "  2.4302844e-05 1.4926190e-06 2.0018380e-04 1.5880600e-05 1.4370199e-03]]\n",
            "============================================================\n",
            "ONNX predicted value: 4\n",
            "Correct prediction\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y47Ac50ys2eN"
      },
      "source": [
        "# Comaprision of inference time and model size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msJMtblfs74C",
        "outputId": "1dc666d2-7a2c-4748-e531-84b6b2b6f599",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        }
      },
      "source": [
        "plt.figure(figsize=(5, 10))\n",
        "sns.color_palette(\"husl\", 8)\n",
        "sns.barplot(\n",
        "    [\"raw keras\", \"onnx\"],\n",
        "    [keras_time_100it, onnx_time_100it],\n",
        ")\n",
        "\n",
        "\n",
        "plt.title(f\"ratio: {keras_time_100it/onnx_time_100it}\")\n",
        "plt.show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onzQ8cGoyQL6",
        "outputId": "7c590430-c9c8-4fad-a500-7ba30197dad1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(onnxs_prbs_100it)\n",
        "print(keras_prbs_100it)\n",
        "\n",
        "# Note no change in prediction probabs"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.0713405e-07 2.5377294e-05 2.8030775e-07 1.8252522e-05 9.9827707e-01\n",
            "  2.4302844e-05 1.4926190e-06 2.0018380e-04 1.5880600e-05 1.4370199e-03]]\n",
            "[[1.0713426e-07 2.5377343e-05 2.8030829e-07 1.8252522e-05 9.9827707e-01\n",
            "  2.4302868e-05 1.4926248e-06 2.0018418e-04 1.5880645e-05 1.4370248e-03]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWDBSU1ge-aW",
        "outputId": "41eb0716-8d97-4f9f-e098-1f8fef6d4885",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def get_size(model_file_path='saved_model.pb'):\n",
        "  size = os.path.getsize(model_file_path)\n",
        "  return round(size/(1024.0),3)\n",
        "\n",
        "print(\"Raw keras .h5 \\t:\", get_size('keras_saved_model/model.h5'), \"KB\")\n",
        "print(\"onnx optimized \\t:\", get_size('keras-mnist-optimized.onnx'), \"KB\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Raw keras .h5 \t: 443.859 KB\n",
            "onnx optimized \t: 138.676 KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anWanNmmgig_"
      },
      "source": [
        "- No change in number of parameters\n",
        "- No change is prediction probabilities\n",
        "- Speed increased sustantially\n",
        "- Size decreased substantially "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLQ-XaltqxQx"
      },
      "source": [
        "# **Part 2:** Let's Get Heavy\n",
        "\n",
        "- Pretrained VGG16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bx7_TK9SDA4w"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}