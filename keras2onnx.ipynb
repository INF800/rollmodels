{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "keras2onnx.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOiVwMGzAxN1I4phegyOrxI",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rakesh4real/rollmodels/blob/main/keras2onnx.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67bo14f6L4XC"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reERZLldLrnQ",
        "outputId": "c512022a-f4a0-4d93-c50d-89ef455d5930",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# original source: https://github.com/onnx/keras-onnx/tree/master/tutorial\n",
        "# Note: this code is compact and better compared to original\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "## 1. Setup\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "# 2. Random Image\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def get_and_plot_random_image_from(x_test, y_test):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    # get a random image index from the test set\n",
        "    image_index = int(np.random.randint(0, x_test.shape[0], size=1)[0])\n",
        "    expected_label = y_test[image_index]\n",
        "    digit_image = x_test[image_index]\n",
        "    # and plot it\n",
        "    plt.title(f'Example {image_index} Label: {expected_label}')\n",
        "    plt.imshow(digit_image.squeeze(2), cmap='Greys')\n",
        "    plt.show()\n",
        "    return digit_image, image_index\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "# 3. Inference w/o onnx\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def infer_w_raw_h5(digit_image, expected_label, model, input_shape, loop_count=100):\n",
        "    # reshape the image for inference/prediction\n",
        "    #digit_image = digit_image.reshape(1, input_shape[0], input_shape[1], 1)\n",
        "    digit_image = digit_image.reshape(1, input_shape[0], input_shape[1], input_shape[2])\n",
        "\n",
        "    # loop `loop_count` times\n",
        "    start_time = time.time()\n",
        "    for i in range(loop_count):\n",
        "        prediction_probabs = model.predict(digit_image)\n",
        "    avg_time = ((time.time() - start_time) / loop_count)\n",
        "    print(f\"Keras inferences with {avg_time} second in average\")\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"Prediction probabilities:\")\n",
        "    print(prediction_probabs)\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    predicted_label = prediction_probabs.argmax()\n",
        "    print('Predicted value:', predicted_label)\n",
        "\n",
        "    is_correct_pred = None\n",
        "    if (expected_label.argmax() == predicted_label):\n",
        "      print('Correct prediction')\n",
        "      is_correct_pred = True\n",
        "    else:\n",
        "      print('Wrong prediction')\n",
        "      is_correct_pred = False\n",
        "\n",
        "    # can be used for more control (see suggestio where used)\n",
        "    return (is_correct_pred, prediction_probabs), avg_time\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "# 4. Inference w/ onnx\n",
        "\n",
        "!pip install --quiet -U onnxruntime\n",
        "!pip install --quiet -U git+https://github.com/microsoft/onnxconverter-common\n",
        "!pip install --quiet -U git+https://github.com/onnx/keras-onnx\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "!pip install --quiet -U onnxruntime\n",
        "!pip install --quiet -U git+https://github.com/microsoft/onnxconverter-common\n",
        "!pip install --quiet -U git+https://github.com/onnx/keras-onnx\n",
        "\n",
        "import onnxruntime\n",
        "import keras2onnx\n",
        "\n",
        "def h52onnx(model, output_onnxmodel_path='./onnx_model.onnx'):\n",
        "  print(\"keras2onnx version is \"+keras2onnx.__version__)\n",
        "  save_name = output_onnxmodel_path.split(\".onnx\")[0].split(\"/\")[-1] # get name from path\n",
        "  print(save_name)\n",
        "  # convert to onnx model\n",
        "  onnx_model = keras2onnx.convert_keras(model, save_name, debug_mode=1)\n",
        "  # and save the model in ONNX format\n",
        "  keras2onnx.save_model(onnx_model, output_onnxmodel_path)\n",
        "\n",
        "\n",
        "def generate_data_feed_and_sess(digit_image, input_shape, output_onnxmodel_path):\n",
        "    # reshape the image for inference/prediction\n",
        "    digit_image = digit_image.reshape(1, input_shape[0], input_shape[1], 1)\n",
        "    \n",
        "    # define session (include options using docs if necessary)\n",
        "    sess_options = onnxruntime.SessionOptions()\n",
        "    sess = onnxruntime.InferenceSession(output_onnxmodel_path, sess_options)\n",
        "    # define data\n",
        "    data = [digit_image.astype(np.float32)]\n",
        "    # feed data\n",
        "    input_names = sess.get_inputs()\n",
        "    feed = dict([(input.name, data[n]) for n, input in enumerate(sess.get_inputs())])\n",
        "    return feed, sess\n",
        "    \n",
        "\n",
        "def infer_w_onnx_runtime(digit_image, expected_label,\n",
        "                         input_shape,\n",
        "                         output_onnxmodel_path='./onnx_model.onnx',\n",
        "                         loop_count=100):\n",
        "  # setup\n",
        "  feed, sess = generate_data_feed_and_sess(digit_image, input_shape, output_onnxmodel_path)\n",
        "\n",
        "  # calculate average\n",
        "  start_time = time.time()\n",
        "  for i in range(loop_count):\n",
        "      # note: session is used to make preds\n",
        "      onnx_prediction_probabs = sess.run(None, feed)[0]\n",
        "  avg_time = ((time.time() - start_time) / loop_count)\n",
        "  print(f\"ONNX inferences with {avg_time} second in average\")\n",
        "\n",
        "  # compare w/ raw h5\n",
        "  print(\"=\"*60)\n",
        "  print(\"[onnx]Prediction probabilities:\")\n",
        "  print(onnx_prediction_probabs)\n",
        "  print(\"=\"*60)\n",
        "\n",
        "  is_correct_pred = None\n",
        "  print('ONNX predicted value:', onnx_prediction_probabs.argmax())\n",
        "  if (expected_label.argmax() == onnx_prediction_probabs.argmax()):\n",
        "    print('Correct prediction')\n",
        "    is_correct_pred = True\n",
        "  else:\n",
        "    print('Wrong prediction')\n",
        "    is_correct_pred = False\n",
        "\n",
        "  return (is_correct_pred, onnx_prediction_probabs), avg_time"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 3.8MB 2.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.4MB 2.6MB/s \n",
            "\u001b[?25h  Building wheel for onnxconverter-common (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 81kB 2.3MB/s \n",
            "\u001b[?25h  Building wheel for keras2onnx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQoS0fs-qoCa"
      },
      "source": [
        "# **Part 1:** Lighter Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiVPGdNYCVRl"
      },
      "source": [
        "# Create Keras .h5 Model\n",
        "\n",
        "- Makes sure input dims is given in first layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jd80L3VIgmBw"
      },
      "source": [
        "input_shape = (28, 28, 1)\n",
        "\n",
        "# Important while creating keras model, making inference w and w/o keras (onnx)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26E1zR2P_cUX",
        "outputId": "97b8902f-b5e1-4645-f832-1f7368dedcaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "\"\"\"\n",
        "## Setup\n",
        "\"\"\"\n",
        "import os\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow\n",
        "print(\"TensorFlow version is \"+tensorflow.__version__)\n",
        "\n",
        "\n",
        "SEED = 101\n",
        "from numpy.random import seed\n",
        "seed(101)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(101)\n",
        "\n",
        "\"\"\"\n",
        "## Prepare the data\n",
        "\"\"\"\n",
        "\n",
        "# Model / data parameters\n",
        "num_classes = 10\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Scale images to the [0, 1] range\n",
        "x_train = x_train.astype(\"float32\") / 255\n",
        "x_test = x_test.astype(\"float32\") / 255\n",
        "# Make sure images have shape (28, 28, 1)\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(x_train.shape[0], \"train samples\")\n",
        "print(x_test.shape[0], \"test samples\")\n",
        "\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = tensorflow.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tensorflow.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\"\"\"\n",
        "## Build the model\n",
        "\"\"\"\n",
        "\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=input_shape), # !NOTE: input shape must be defined @beg to avoid onnx errors\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\"\"\"\n",
        "## Train the model\n",
        "\"\"\"\n",
        "\n",
        "os.makedirs(\"keras_saved_model\", exist_ok=True)\n",
        "checkpoint_filepath = 'keras_saved_model/model.h5'\n",
        "model_checkpoint_callback = tensorflow.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=False, # else, error\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "\n",
        "\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 10\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=[model_checkpoint_callback])\n",
        "\n",
        "\"\"\"\n",
        "## Evaluate the trained model\n",
        "\"\"\"\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow version is 2.3.0\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                16010     \n",
            "=================================================================\n",
            "Total params: 34,826\n",
            "Trainable params: 34,826\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.3709 - accuracy: 0.8863 - val_loss: 0.0907 - val_accuracy: 0.9770\n",
            "Epoch 2/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.1201 - accuracy: 0.9634 - val_loss: 0.0606 - val_accuracy: 0.9840\n",
            "Epoch 3/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0862 - accuracy: 0.9734 - val_loss: 0.0490 - val_accuracy: 0.9868\n",
            "Epoch 4/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0733 - accuracy: 0.9771 - val_loss: 0.0443 - val_accuracy: 0.9870\n",
            "Epoch 5/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0624 - accuracy: 0.9807 - val_loss: 0.0378 - val_accuracy: 0.9902\n",
            "Epoch 6/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0561 - accuracy: 0.9828 - val_loss: 0.0385 - val_accuracy: 0.9895\n",
            "Epoch 7/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0520 - accuracy: 0.9841 - val_loss: 0.0346 - val_accuracy: 0.9912\n",
            "Epoch 8/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0466 - accuracy: 0.9858 - val_loss: 0.0349 - val_accuracy: 0.9910\n",
            "Epoch 9/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0451 - accuracy: 0.9857 - val_loss: 0.0347 - val_accuracy: 0.9908\n",
            "Epoch 10/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0417 - accuracy: 0.9868 - val_loss: 0.0320 - val_accuracy: 0.9912\n",
            "Test loss: 0.03029739297926426\n",
            "Test accuracy: 0.989799976348877\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwfkAOeEDfr_"
      },
      "source": [
        "# Load and eval best.h5 model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXOnOTUQDJwr",
        "outputId": "cd72267c-8aa9-47c6-8021-8fe838982cae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# best models\n",
        "model = tensorflow.keras.models.load_model(checkpoint_filepath)\n",
        "\n",
        "# eval\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.030320575460791588\n",
            "Test accuracy: 0.9894999861717224\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8gXaGUhHZ3a",
        "outputId": "7f6256b4-110c-4df2-fcd3-cc1d525279ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        }
      },
      "source": [
        "# a. get random image\n",
        "digit_image, digit_idx = get_and_plot_random_image_from(x_test, y_test)\n",
        "\n",
        "# b. loop inference for avg. inference time\n",
        "(_, keras_prbs_100it), keras_time_100it = infer_w_raw_h5(digit_image, expected_label=y_test[digit_idx],\n",
        "                                                              model=model, input_shape=input_shape)\n",
        "\n",
        "# SUGGESTION: For more control, as this is for single, image, \n",
        "# Go one step further by putiing the above two functions \n",
        "# in loop as well and take average"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Keras inferences with 0.03954606294631958 second in average\n",
            "============================================================\n",
            "Prediction probabilities:\n",
            "[[9.5465268e-08 2.4304649e-05 2.8437185e-07 1.7355527e-05 9.9794000e-01\n",
            "  3.2436950e-05 2.1096369e-06 2.4113405e-04 1.8779225e-05 1.7233961e-03]]\n",
            "============================================================\n",
            "Predicted value: 4\n",
            "Correct prediction\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RI36w9_PJaqP"
      },
      "source": [
        "# Conversion from Keras to ONNX\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pJG8tvOJeE2",
        "outputId": "802f05b4-25e4-4fa5-beb3-6c02310c5733",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "h52onnx(model, \"./keras-mnist-optimized.onnx\") # saves in curdir"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf executing eager_mode: True\n",
            "tf.keras model eager_mode: False\n",
            "Processing a keras layer - (dense: <class 'tensorflow.python.keras.layers.core.Dense'>)\n",
            "\toutput: dense/Softmax_1:0\n",
            "\tinput : dropout/cond_1/Identity:0\n",
            "Processing a keras layer - (dropout: <class 'tensorflow.python.keras.layers.core.Dropout'>)\n",
            "\toutput: dropout/cond_1/Identity:0\n",
            "\tinput : flatten/Reshape_1:0\n",
            "Processing a keras layer - (flatten: <class 'tensorflow.python.keras.layers.core.Flatten'>)\n",
            "\toutput: flatten/Reshape_1:0\n",
            "\tinput : max_pooling2d_1/MaxPool_1:0\n",
            "Processing a keras layer - (max_pooling2d_1: <class 'tensorflow.python.keras.layers.pooling.MaxPooling2D'>)\n",
            "\toutput: max_pooling2d_1/MaxPool_1:0\n",
            "\tinput : conv2d_1/Relu_1:0\n",
            "Processing a keras layer - (conv2d_1: <class 'tensorflow.python.keras.layers.convolutional.Conv2D'>)\n",
            "\toutput: conv2d_1/Relu_1:0\n",
            "\tinput : max_pooling2d/MaxPool_1:0\n",
            "Processing a keras layer - (max_pooling2d: <class 'tensorflow.python.keras.layers.pooling.MaxPooling2D'>)\n",
            "\toutput: max_pooling2d/MaxPool_1:0\n",
            "\tinput : conv2d/Relu_1:0\n",
            "Processing a keras layer - (conv2d: <class 'tensorflow.python.keras.layers.convolutional.Conv2D'>)\n",
            "\toutput: conv2d/Relu_1:0\n",
            "\tinput : input_1_1:0\n",
            "var: input_1\n",
            "var: input_1_1:0\n",
            "var: input_1_1:01\n",
            "var: conv2d/Relu_1:0\n",
            "var: max_pooling2d/MaxPool_1:0\n",
            "var: conv2d_1/Relu_1:0\n",
            "var: max_pooling2d_1/MaxPool_1:0\n",
            "var: flatten/Reshape_1:0\n",
            "var: dropout/cond_1/Identity:0\n",
            "var: dense/Softmax_1:01\n",
            "var: dense/Softmax_1:0\n",
            "var: dense\n",
            "Converting the operator (Identity): Identity\n",
            "Converting the operator (Identity1): Identity\n",
            "Converting the operator (Identity2): Identity\n",
            "Converting the operator (dense): <class 'tensorflow.python.keras.layers.core.Dense'>\n",
            "Converting the operator (dropout): <class 'tensorflow.python.keras.layers.core.Dropout'>\n",
            "Converting the operator (keras_learning_phase/input): Const\n",
            "Converting the operator (flatten): <class 'tensorflow.python.keras.layers.core.Flatten'>\n",
            "Converting the operator (flatten/Const_1): Const\n",
            "Converting the operator (max_pooling2d_1): <class 'tensorflow.python.keras.layers.pooling.MaxPooling2D'>\n",
            "Converting the operator (conv2d_1): <class 'tensorflow.python.keras.layers.convolutional.Conv2D'>\n",
            "Converting the operator (max_pooling2d): <class 'tensorflow.python.keras.layers.pooling.MaxPooling2D'>\n",
            "Converting the operator (conv2d): <class 'tensorflow.python.keras.layers.convolutional.Conv2D'>\n",
            "Converting the operator (Identity3): Identity\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "keras2onnx version is 1.7.1\n",
            "keras-mnist-optimized\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                16010     \n",
            "=================================================================\n",
            "Total params: 34,826\n",
            "Trainable params: 34,826\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8UkDSbKJxIJ"
      },
      "source": [
        "# Evaluate the ONNX's Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOgM8jEvjY02",
        "outputId": "b7a599c6-a273-48fd-88bd-aa0e646127c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "(_, onnxs_prbs_100it), onnx_time_100it = infer_w_onnx_runtime(digit_image, expected_label=y_test[digit_idx], \n",
        "                                                              input_shape=input_shape,\n",
        "                                                              output_onnxmodel_path='./keras-mnist-optimized.onnx') # same path as used above to load"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ONNX inferences with 0.0005868148803710938 second in average\n",
            "============================================================\n",
            "[onnx]Prediction probabilities:\n",
            "[[9.5465268e-08 2.4304694e-05 2.8437185e-07 1.7355558e-05 9.9794000e-01\n",
            "  3.2436947e-05 2.1096446e-06 2.4113450e-04 1.8779225e-05 1.7234019e-03]]\n",
            "============================================================\n",
            "ONNX predicted value: 4\n",
            "Correct prediction\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y47Ac50ys2eN"
      },
      "source": [
        "# Comaprision of inference time and model size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msJMtblfs74C",
        "outputId": "d95c6c5c-29a5-410a-fae5-2bd31b3895a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        }
      },
      "source": [
        "plt.figure(figsize=(5, 10))\n",
        "sns.color_palette(\"husl\", 8)\n",
        "sns.barplot(\n",
        "    [\"raw keras\", \"onnx\"],\n",
        "    [keras_time_100it, onnx_time_100it],\n",
        ")\n",
        "\n",
        "\n",
        "plt.title(f\"ratio: {keras_time_100it/onnx_time_100it}\")\n",
        "plt.show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onzQ8cGoyQL6",
        "outputId": "fb296271-5d25-4b9c-f0df-b5d37ec0b9ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(onnxs_prbs_100it)\n",
        "print(keras_prbs_100it)\n",
        "\n",
        "# Note no change in prediction probabs"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[9.5465268e-08 2.4304694e-05 2.8437185e-07 1.7355558e-05 9.9794000e-01\n",
            "  3.2436947e-05 2.1096446e-06 2.4113450e-04 1.8779225e-05 1.7234019e-03]]\n",
            "[[9.5465268e-08 2.4304649e-05 2.8437185e-07 1.7355527e-05 9.9794000e-01\n",
            "  3.2436950e-05 2.1096369e-06 2.4113405e-04 1.8779225e-05 1.7233961e-03]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWDBSU1ge-aW",
        "outputId": "e1b6e9a0-fa60-4df9-f897-7842ff360ff2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def get_size(model_file_path='saved_model.pb'):\n",
        "  size = os.path.getsize(model_file_path)\n",
        "  return round(size/(1024.0),3)\n",
        "\n",
        "print(\"Raw keras .h5 \\t:\", get_size('keras_saved_model/model.h5'), \"KB\")\n",
        "print(\"onnx optimized \\t:\", get_size('keras-mnist-optimized.onnx'), \"KB\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Raw keras .h5 \t: 443.758 KB\n",
            "onnx optimized \t: 138.607 KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anWanNmmgig_"
      },
      "source": [
        "- No change in number of parameters\n",
        "- No change is prediction probabilities\n",
        "- Speed increased by **50 times**\n",
        "- Size decreased by **3 times** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLQ-XaltqxQx"
      },
      "source": [
        "# **Part 2:** Let's Get Heavy\n",
        "\n",
        "- Pretrained EfficientNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFJMWmkMvMBp",
        "outputId": "23b60f08-07a9-4201-9505-f26b8b7a7910",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        }
      },
      "source": [
        "https://github.com/ardamavi/Dog-Cat-Classifier/blob/master/Data/Model/weights.h5"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-d565d23f2895>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    https://github.com/ardamavi/Dog-Cat-Classifier/blob/master/Data/Model/weights.h5\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpnzbQT5vlYO"
      },
      "source": [
        "import numpy as np\n",
        "import efficientnet.tfkeras as efn\n",
        "from tensorflow.keras.applications.imagenet_utils import decode_predictions, preprocess_input\n",
        "from efficientnet.preprocessing import center_crop_and_resize\n",
        "from skimage.io import imread"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwIz9cLfvv6k"
      },
      "source": [
        "model = efn.EfficientNetB0(weights='imagenet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMuZAd4OwXLv"
      },
      "source": [
        "print(cv2.imread('cat.jpg', 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12O7QEMqvwwt"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "image = cv2.imread('./cat.jpg')\n",
        "print(image.shape)\n",
        "\n",
        "image_size = model.input_shape[1]\n",
        "plt.imshow(image, interpolation='nearest')\n",
        "plt.show()\n",
        "x = center_crop_and_resize(image, image_size=image_size)\n",
        "\n",
        "x = preprocess_input(x, mode='torch')\n",
        "inputs = np.expand_dims(x, 0)\n",
        "expected = model.predict(inputs)\n",
        "decode_predictions(expected)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRZ21CIvv3Nc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}