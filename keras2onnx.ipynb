{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "keras2onnx.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPxPW9nLcIHHwtz4Rh71t1/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rakesh4real/rollmodels/blob/main/keras2onnx.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67bo14f6L4XC"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reERZLldLrnQ",
        "outputId": "720eb044-99a8-490e-d6d7-8238580e6d77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# original source: https://github.com/onnx/keras-onnx/tree/master/tutorial\n",
        "# Note: this code is compact and better compared to original\n",
        "\n",
        "\"\"\"\n",
        "## 1. Setup\n",
        "\"\"\"\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import seaborn as sns\n",
        "\n",
        "\"\"\"\n",
        "# 2. Random Image\n",
        "\"\"\"\n",
        "def get_and_plot_random_image_from(x_test, y_test):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    # get a random image index from the test set\n",
        "    image_index = int(np.random.randint(0, x_test.shape[0], size=1)[0])\n",
        "    expected_label = y_test[image_index]\n",
        "    digit_image = x_test[image_index]\n",
        "    # and plot it\n",
        "    plt.title(f'Example {image_index} Label: {expected_label}')\n",
        "    plt.imshow(digit_image.squeeze(2), cmap='Greys')\n",
        "    plt.show()\n",
        "    return digit_image, image_index\n",
        "\n",
        "\"\"\"\n",
        "# 3. Inference w/o onnx\n",
        "\"\"\"\n",
        "def infer_w_raw_h5(digit_image, expected_label, model, input_shape, loop_count=40):\n",
        "    # reshape the image for inference/prediction\n",
        "    #digit_image = digit_image.reshape(1, input_shape[0], input_shape[1], 1)\n",
        "    digit_image = digit_image.reshape(1, input_shape[0], input_shape[1], input_shape[2])\n",
        "\n",
        "    # loop `loop_count` times\n",
        "    start_time = time.time()\n",
        "    for i in range(loop_count):\n",
        "        prediction_probabs = model.predict(digit_image)\n",
        "    avg_time = ((time.time() - start_time) / loop_count)\n",
        "    print(f\"Keras inferences with {avg_time} second in average\")\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"Prediction probabilities:\")\n",
        "    print(prediction_probabs)\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    predicted_label = prediction_probabs.argmax()\n",
        "    print('Predicted value:', predicted_label)\n",
        "\n",
        "    is_correct_pred = None\n",
        "    if (expected_label.argmax() == predicted_label):\n",
        "      print('Correct prediction')\n",
        "      is_correct_pred = True\n",
        "    else:\n",
        "      print('Wrong prediction')\n",
        "      is_correct_pred = False\n",
        "\n",
        "    # can be used for more control (see suggestio where used)\n",
        "    return (is_correct_pred, prediction_probabs), avg_time\n",
        "\n",
        "\"\"\"\n",
        "# 4. Inference w/ onnx\n",
        "\n",
        "!pip install --quiet -U onnxruntime\n",
        "!pip install --quiet -U git+https://github.com/microsoft/onnxconverter-common\n",
        "!pip install --quiet -U git+https://github.com/onnx/keras-onnx\n",
        "\"\"\"\n",
        "!pip install --quiet -U onnxruntime\n",
        "!pip install --quiet -U git+https://github.com/microsoft/onnxconverter-common\n",
        "!pip install --quiet -U git+https://github.com/onnx/keras-onnx\n",
        "\n",
        "import onnxruntime\n",
        "import keras2onnx\n",
        "\n",
        "def h52onnx(model, output_onnxmodel_path='./onnx_model.onnx'):\n",
        "  print(\"keras2onnx version is \"+keras2onnx.__version__)\n",
        "  save_name = output_onnxmodel_path.split(\".onnx\")[0].split(\"/\")[-1] # get name from path\n",
        "  print(save_name)\n",
        "  # convert to onnx model\n",
        "  onnx_model = keras2onnx.convert_keras(model, save_name, debug_mode=1)\n",
        "  # and save the model in ONNX format\n",
        "  keras2onnx.save_model(onnx_model, output_onnxmodel_path)\n",
        "\n",
        "\n",
        "def generate_data_feed_and_sess(digit_image, input_shape, output_onnxmodel_path):\n",
        "    # reshape the image for inference/prediction\n",
        "    digit_image = digit_image.reshape(1, input_shape[0], input_shape[1], 1)\n",
        "    \n",
        "    # define session (include options using docs if necessary)\n",
        "    sess_options = onnxruntime.SessionOptions()\n",
        "    sess = onnxruntime.InferenceSession(output_onnxmodel_path, sess_options)\n",
        "    # define data\n",
        "    data = [digit_image.astype(np.float32)]\n",
        "    # feed data\n",
        "    input_names = sess.get_inputs()\n",
        "    feed = dict([(input.name, data[n]) for n, input in enumerate(sess.get_inputs())])\n",
        "    return feed, sess\n",
        "    \n",
        "\n",
        "def infer_w_onnx_runtime(digit_image, expected_label,\n",
        "                         input_shape,\n",
        "                         output_onnxmodel_path='./onnx_model.onnx',\n",
        "                         loop_count=40):\n",
        "  # setup\n",
        "  feed, sess = generate_data_feed_and_sess(digit_image, input_shape, output_onnxmodel_path)\n",
        "\n",
        "  # calculate average\n",
        "  start_time = time.time()\n",
        "  for i in range(loop_count):\n",
        "      # note: session is used to make preds\n",
        "      onnx_prediction_probabs = sess.run(None, feed)[0]\n",
        "  avg_time = ((time.time() - start_time) / loop_count)\n",
        "  print(f\"ONNX inferences with {avg_time} second in average\")\n",
        "\n",
        "  # compare w/ raw h5\n",
        "  print(\"=\"*60)\n",
        "  print(\"[onnx]Prediction probabilities:\")\n",
        "  print(onnx_prediction_probabs)\n",
        "  print(\"=\"*60)\n",
        "\n",
        "  is_correct_pred = None\n",
        "  print('ONNX predicted value:', onnx_prediction_probabs.argmax())\n",
        "  if (expected_label.argmax() == onnx_prediction_probabs.argmax()):\n",
        "    print('Correct prediction')\n",
        "    is_correct_pred = True\n",
        "  else:\n",
        "    print('Wrong prediction')\n",
        "    is_correct_pred = False\n",
        "\n",
        "  return (is_correct_pred, onnx_prediction_probabs), avg_time"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 3.8MB 2.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.4MB 2.4MB/s \n",
            "\u001b[?25h  Building wheel for onnxconverter-common (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 81kB 2.1MB/s \n",
            "\u001b[?25h  Building wheel for keras2onnx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQoS0fs-qoCa"
      },
      "source": [
        "# **Part 1:** Lighter Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiVPGdNYCVRl"
      },
      "source": [
        "# Create Keras .h5 Model\n",
        "\n",
        "- Makes sure input dims is given in first layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jd80L3VIgmBw"
      },
      "source": [
        "input_shape = (28, 28, 1)\n",
        "\n",
        "# Important while creating keras model, making inference w and w/o keras (onnx)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26E1zR2P_cUX",
        "outputId": "77b390b3-f630-4e06-f74a-c15eb521b2df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        }
      },
      "source": [
        "\"\"\"\n",
        "## Setup\n",
        "\"\"\"\n",
        "import os\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow\n",
        "print(\"TensorFlow version is \"+tensorflow.__version__)\n",
        "\n",
        "\"\"\"\n",
        "## Prepare the data\n",
        "\"\"\"\n",
        "\n",
        "# Model / data parameters\n",
        "num_classes = 10\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Scale images to the [0, 1] range\n",
        "x_train = x_train.astype(\"float32\") / 255\n",
        "x_test = x_test.astype(\"float32\") / 255\n",
        "# Make sure images have shape (28, 28, 1)\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(x_train.shape[0], \"train samples\")\n",
        "print(x_test.shape[0], \"test samples\")\n",
        "\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = tensorflow.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tensorflow.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\"\"\"\n",
        "## Build the model\n",
        "\"\"\"\n",
        "\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=input_shape), # !NOTE: input shape must be defined @beg to avoid onnx errors\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\"\"\"\n",
        "## Train the model\n",
        "\"\"\"\n",
        "\n",
        "os.makedirs(\"keras_saved_model\", exist_ok=True)\n",
        "checkpoint_filepath = 'keras_saved_model/model.h5'\n",
        "model_checkpoint_callback = tensorflow.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=False, # else, error\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "\n",
        "\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 10\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=[model_checkpoint_callback])\n",
        "\n",
        "\"\"\"\n",
        "## Evaluate the trained model\n",
        "\"\"\"\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow version is 2.3.0\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                16010     \n",
            "=================================================================\n",
            "Total params: 34,826\n",
            "Trainable params: 34,826\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.3794 - accuracy: 0.8843 - val_loss: 0.0833 - val_accuracy: 0.9790\n",
            "Epoch 2/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.1135 - accuracy: 0.9648 - val_loss: 0.0565 - val_accuracy: 0.9848\n",
            "Epoch 3/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0861 - accuracy: 0.9741 - val_loss: 0.0434 - val_accuracy: 0.9883\n",
            "Epoch 4/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0704 - accuracy: 0.9782 - val_loss: 0.0413 - val_accuracy: 0.9885\n",
            "Epoch 5/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0634 - accuracy: 0.9807 - val_loss: 0.0346 - val_accuracy: 0.9907\n",
            "Epoch 6/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0556 - accuracy: 0.9825 - val_loss: 0.0349 - val_accuracy: 0.9903\n",
            "Epoch 7/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0510 - accuracy: 0.9838 - val_loss: 0.0367 - val_accuracy: 0.9900\n",
            "Epoch 8/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0495 - accuracy: 0.9843 - val_loss: 0.0344 - val_accuracy: 0.9907\n",
            "Epoch 9/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0455 - accuracy: 0.9856 - val_loss: 0.0319 - val_accuracy: 0.9903\n",
            "Epoch 10/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0414 - accuracy: 0.9871 - val_loss: 0.0298 - val_accuracy: 0.9918\n",
            "Test loss: 0.029914841055870056\n",
            "Test accuracy: 0.9894999861717224\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwfkAOeEDfr_"
      },
      "source": [
        "# Load and eval best.h5 model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXOnOTUQDJwr",
        "outputId": "6202fa39-3fc6-41e2-f577-1204657e00e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# best models\n",
        "model = tensorflow.keras.models.load_model(checkpoint_filepath)\n",
        "\n",
        "# eval\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.029914841055870056\n",
            "Test accuracy: 0.9894999861717224\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8gXaGUhHZ3a",
        "outputId": "abec2137-b815-4593-a3ee-9f142e7c3b33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        }
      },
      "source": [
        "# a. get random image\n",
        "digit_image, digit_idx = get_and_plot_random_image_from(x_test, y_test)\n",
        "\n",
        "# b. loop inference for avg. inference time\n",
        "(_, keras_prbs_40it), keras_time_40it = infer_w_raw_h5(digit_image, expected_label=y_test[digit_idx],\n",
        "                                                              model=model, input_shape=input_shape)\n",
        "\n",
        "# SUGGESTION: For more control, as this is for single, image, \n",
        "# Go one step further by putiing the above two functions \n",
        "# in loop as well and take average"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Keras inferences with 0.0415363073348999 second in average\n",
            "============================================================\n",
            "Prediction probabilities:\n",
            "[[2.4790088e-08 1.3183645e-06 4.4287890e-07 4.1017855e-05 5.7662021e-07\n",
            "  2.5935281e-06 4.8100645e-11 9.9991345e-01 3.8079608e-05 2.4827425e-06]]\n",
            "============================================================\n",
            "Predicted value: 7\n",
            "Correct prediction\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RI36w9_PJaqP"
      },
      "source": [
        "# Conversion from Keras to ONNX\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pJG8tvOJeE2",
        "outputId": "50225250-2685-4b1f-d2ef-30f76d0abcba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "h52onnx(model, \"./keras-mnist-optimized.onnx\") # saves in curdir"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf executing eager_mode: True\n",
            "tf.keras model eager_mode: False\n",
            "Processing a keras layer - (dense: <class 'tensorflow.python.keras.layers.core.Dense'>)\n",
            "\toutput: dense/Softmax_1:0\n",
            "\tinput : dropout/cond_1/Identity:0\n",
            "Processing a keras layer - (dropout: <class 'tensorflow.python.keras.layers.core.Dropout'>)\n",
            "\toutput: dropout/cond_1/Identity:0\n",
            "\tinput : flatten/Reshape_1:0\n",
            "Processing a keras layer - (flatten: <class 'tensorflow.python.keras.layers.core.Flatten'>)\n",
            "\toutput: flatten/Reshape_1:0\n",
            "\tinput : max_pooling2d_1/MaxPool_1:0\n",
            "Processing a keras layer - (max_pooling2d_1: <class 'tensorflow.python.keras.layers.pooling.MaxPooling2D'>)\n",
            "\toutput: max_pooling2d_1/MaxPool_1:0\n",
            "\tinput : conv2d_1/Relu_1:0\n",
            "Processing a keras layer - (conv2d_1: <class 'tensorflow.python.keras.layers.convolutional.Conv2D'>)\n",
            "\toutput: conv2d_1/Relu_1:0\n",
            "\tinput : max_pooling2d/MaxPool_1:0\n",
            "Processing a keras layer - (max_pooling2d: <class 'tensorflow.python.keras.layers.pooling.MaxPooling2D'>)\n",
            "\toutput: max_pooling2d/MaxPool_1:0\n",
            "\tinput : conv2d/Relu_1:0\n",
            "Processing a keras layer - (conv2d: <class 'tensorflow.python.keras.layers.convolutional.Conv2D'>)\n",
            "\toutput: conv2d/Relu_1:0\n",
            "\tinput : input_1_1:0\n",
            "var: input_1\n",
            "var: input_1_1:0\n",
            "var: input_1_1:01\n",
            "var: conv2d/Relu_1:0\n",
            "var: max_pooling2d/MaxPool_1:0\n",
            "var: conv2d_1/Relu_1:0\n",
            "var: max_pooling2d_1/MaxPool_1:0\n",
            "var: flatten/Reshape_1:0\n",
            "var: dropout/cond_1/Identity:0\n",
            "var: dense/Softmax_1:01\n",
            "var: dense/Softmax_1:0\n",
            "var: dense\n",
            "Converting the operator (Identity): Identity\n",
            "Converting the operator (Identity1): Identity\n",
            "Converting the operator (Identity2): Identity\n",
            "Converting the operator (dense): <class 'tensorflow.python.keras.layers.core.Dense'>\n",
            "Converting the operator (dropout): <class 'tensorflow.python.keras.layers.core.Dropout'>\n",
            "Converting the operator (keras_learning_phase/input): Const\n",
            "Converting the operator (flatten): <class 'tensorflow.python.keras.layers.core.Flatten'>\n",
            "Converting the operator (flatten/Const_1): Const\n",
            "Converting the operator (max_pooling2d_1): <class 'tensorflow.python.keras.layers.pooling.MaxPooling2D'>\n",
            "Converting the operator (conv2d_1): <class 'tensorflow.python.keras.layers.convolutional.Conv2D'>\n",
            "Converting the operator (max_pooling2d): <class 'tensorflow.python.keras.layers.pooling.MaxPooling2D'>\n",
            "Converting the operator (conv2d): <class 'tensorflow.python.keras.layers.convolutional.Conv2D'>\n",
            "Converting the operator (Identity3): Identity\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "keras2onnx version is 1.7.1\n",
            "keras-mnist-optimized\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                16010     \n",
            "=================================================================\n",
            "Total params: 34,826\n",
            "Trainable params: 34,826\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8UkDSbKJxIJ"
      },
      "source": [
        "# Evaluate the ONNX's Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOgM8jEvjY02",
        "outputId": "5dd92fe0-791d-4631-9e3c-10e1b9d213c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "(_, onnxs_prbs_40it), onnx_time_40it = infer_w_onnx_runtime(digit_image, expected_label=y_test[digit_idx], \n",
        "                                                              input_shape=input_shape,\n",
        "                                                              output_onnxmodel_path='./keras-mnist-optimized.onnx') # same path as used above to load"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ONNX inferences with 0.0017925798892974854 second in average\n",
            "============================================================\n",
            "[onnx]Prediction probabilities:\n",
            "[[2.4790136e-08 1.3183620e-06 4.4287893e-07 4.1017815e-05 5.7661913e-07\n",
            "  2.5935233e-06 4.8100555e-11 9.9991345e-01 3.8079575e-05 2.4827402e-06]]\n",
            "============================================================\n",
            "ONNX predicted value: 7\n",
            "Correct prediction\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y47Ac50ys2eN"
      },
      "source": [
        "# Comaprision of inference time and model size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msJMtblfs74C",
        "outputId": "e7a14da9-eaa0-42d2-d081-c8995af930c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        }
      },
      "source": [
        "plt.figure(figsize=(5, 10))\n",
        "sns.color_palette(\"husl\", 8)\n",
        "sns.barplot(\n",
        "    [\"raw keras\", \"onnx\"],\n",
        "    [keras_time_40it, onnx_time_40it],\n",
        ")\n",
        "\n",
        "plt.title(f\"ratio: {keras_time_40it/onnx_time_40it}\")\n",
        "plt.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onzQ8cGoyQL6",
        "outputId": "7b8e99d8-17cf-4c16-eeec-10da42932456",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(onnxs_prbs_40it)\n",
        "print(keras_prbs_40it)\n",
        "\n",
        "# Note no change in prediction probabs"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2.4790136e-08 1.3183620e-06 4.4287893e-07 4.1017815e-05 5.7661913e-07\n",
            "  2.5935233e-06 4.8100555e-11 9.9991345e-01 3.8079575e-05 2.4827402e-06]]\n",
            "[[2.4790088e-08 1.3183645e-06 4.4287890e-07 4.1017855e-05 5.7662021e-07\n",
            "  2.5935281e-06 4.8100645e-11 9.9991345e-01 3.8079608e-05 2.4827425e-06]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWDBSU1ge-aW",
        "outputId": "73583535-46c8-4c3a-abc4-c9dbe0c5f9bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def get_size(model_file_path='saved_model.pb'):\n",
        "  size = os.path.getsize(model_file_path)\n",
        "  return round(size/(1024.0),3)\n",
        "\n",
        "print(\"Raw keras .h5 \\t:\", get_size('keras_saved_model/model.h5'), \"KB\")\n",
        "print(\"onnx optimized \\t:\", get_size('keras-mnist-optimized.onnx'), \"KB\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Raw keras .h5 \t: 443.758 KB\n",
            "onnx optimized \t: 138.607 KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anWanNmmgig_"
      },
      "source": [
        "- No change in number of parameters\n",
        "- No change is prediction probabilities\n",
        "- Speed increased by **50 times**\n",
        "- Size decreased by **3 times** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLQ-XaltqxQx"
      },
      "source": [
        "# **Part 2:** Let's Get Heavy\n",
        "\n",
        "- Pretrained EfficientNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFJMWmkMvMBp",
        "outputId": "368bb30e-ad6b-49b5-99dd-6b3b74fc0eaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        }
      },
      "source": [
        "!pip install -U git+https://github.com/qubvel/efficientnet\n",
        "!curl -O https://github.com/rakesh4real/Applied-Machine-Learning/blob/master/Resources/hosted/dog.jpg"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/qubvel/efficientnet\n",
            "  Cloning https://github.com/qubvel/efficientnet to /tmp/pip-req-build-ckuo38u_\n",
            "  Running command git clone -q https://github.com/qubvel/efficientnet /tmp/pip-req-build-ckuo38u_\n",
            "Collecting keras_applications<=1.0.8,>=1.0.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 1.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: scikit-image in /usr/local/lib/python3.6/dist-packages (from efficientnet==1.1.1) (0.16.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras_applications<=1.0.8,>=1.0.7->efficientnet==1.1.1) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras_applications<=1.0.8,>=1.0.7->efficientnet==1.1.1) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.1.1) (2.5)\n",
            "Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.1.1) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.1.1) (7.0.0)\n",
            "Requirement already satisfied, skipping upgrade: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.1.1) (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.1.1) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.1.1) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras_applications<=1.0.8,>=1.0.7->efficientnet==1.1.1) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image->efficientnet==1.1.1) (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.1) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.1) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.1) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.1) (1.2.0)\n",
            "Building wheels for collected packages: efficientnet\n",
            "  Building wheel for efficientnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet: filename=efficientnet-1.1.1-cp36-none-any.whl size=18421 sha256=c8ef0ca68de557d0a810f4301ad622db0f1dbb53dce2e6e623731b8337bf9a3e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-6ld72n6h/wheels/64/60/2e/30ebaa76ed1626e86bfb0cc0579b737fdb7d9ff8cb9522663a\n",
            "Successfully built efficientnet\n",
            "Installing collected packages: keras-applications, efficientnet\n",
            "Successfully installed efficientnet-1.1.1 keras-applications-1.0.8\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 82628    0 82628    0     0   209k      0 --:--:-- --:--:-- --:--:--  209k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpnzbQT5vlYO"
      },
      "source": [
        "import numpy as np\n",
        "import efficientnet.tfkeras as efn\n",
        "from tensorflow.keras.applications.imagenet_utils import decode_predictions, preprocess_input\n",
        "from efficientnet.preprocessing import center_crop_and_resize\n",
        "from skimage.io import imread"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwIz9cLfvv6k",
        "outputId": "49eb72d9-b9cc-4832-c699-a31f35c7e6a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "model = efn.EfficientNetB0(weights='imagenet')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/Callidior/keras-applications/releases/download/efficientnet/efficientnet-b0_weights_tf_dim_ordering_tf_kernels_autoaugment.h5\n",
            "21831680/21826536 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMuZAd4OwXLv",
        "outputId": "4e076e70-ba8c-4dfa-bb30-d3613bd0ac1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(cv2.imread('cat.jpg', 1))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12O7QEMqvwwt",
        "outputId": "3e459839-8265-4a83-ec3f-a93cfc62d5de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "image = cv2.imread('./cat.jpg')\n",
        "print(image.shape)\n",
        "\n",
        "image_size = model.input_shape[1]\n",
        "plt.imshow(image, interpolation='nearest')\n",
        "plt.show()\n",
        "x = center_crop_and_resize(image, image_size=image_size)\n",
        "\n",
        "x = preprocess_input(x, mode='torch')\n",
        "inputs = np.expand_dims(x, 0)\n",
        "expected = model.predict(inputs)\n",
        "decode_predictions(expected)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-21fef0a93585>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./cat.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mimage_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRZ21CIvv3Nc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}